{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # For HTTP requests\n",
    "import re  # For regular expressions\n",
    "import os  # For interacting with the operating system\n",
    "import time  # For time-related operations\n",
    "import json  # For handling JSON data\n",
    "import shutil  # For file operations like copy, move, delete, etc.\n",
    "import pdfplumber  # For extracting content from PDF files\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from openpyxl import Workbook  # For creating and manipulating Excel workbooks\n",
    "from urllib.parse import urlparse  # For parsing URLs\n",
    "\n",
    "\n",
    "# Mapping of document types to their corresponding names in the World Bank API\n",
    "FileNameConvertDict = {\n",
    "    'icrr': 'Implementation+Completion+Report+Review',\n",
    "    'icr': \"Implementation+Completion+and+Results+Report\",\n",
    "    'ppar': \"Project+Performance+Assessment+Report\",\n",
    "    'pad': 'Project+Appraisal+Document',\n",
    "    'scd': 'Systematic+Country+Diagnostic',\n",
    "    'isr': 'Implementation+Status+and+Results+Report',\n",
    "    'pd': 'Program+Document',\n",
    "    'esmp': 'Environmental+and+Social+Management+Plan',\n",
    "    'cas': 'Country+Assistance+Strategy+Document',\n",
    "    'cpf': 'Country+Partnership+Framework',\n",
    "    'cpe': 'IEG+Evaluation',\n",
    "    'cen': 'Country+Engagement+Note',\n",
    "    'cren': 'Country+Re-engagement+Note',\n",
    "    'isn': 'Interim+Strategy+Note',\n",
    "    'cpe': 'Country+Program+Evaluation',\n",
    "    'asa': 'Economic+%26+Sector+Work%5EEconomic+%26amp%3B+Sector+Work%5EPublications%5EPublications+%26+Research%5EPublications+%26amp%3B+Research',\n",
    "    'clrr': 'CAS+Completion+Report+Review',\n",
    "    'pp': 'Project+Paper',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class URLBulkDownload(object):\n",
    "    \"\"\"\n",
    "    A class to download multiple files from URLs.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): A dictionary containing the necessary input parameters.\n",
    "                           Required key: 'root_path' (string) - The root path where downloaded files will be stored.\n",
    "                           Optional key: 'urls' (list) - A list of URLs to download files from (default is an empty list).\n",
    "                           Optional key: 'download_version' (string) - The file download version ('txt' or 'binary')\n",
    "                                                                       (default is 'txt').\n",
    "\n",
    "    Attributes:\n",
    "        root_path (string): The root path where downloaded files will be stored.\n",
    "        urls (list): A list of URLs to download files from.\n",
    "        download_version (string): The file download version ('txt' or 'binary').\n",
    "        output_path (string): The path where downloaded files will be saved under the 'URL_downloads' directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dict):\n",
    "        self.root_path = input_dict['root_path']\n",
    "        self.urls = input_dict.get('urls', [])\n",
    "        self.download_version = input_dict.get('download_version', 'txt')\n",
    "        self.output_path = os.path.join(self.root_path, \"URL_downloads\")\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "\n",
    "    def download_files(self):\n",
    "        \"\"\"\n",
    "        Downloads files from the URLs specified in the 'urls' attribute.\n",
    "\n",
    "        It saves the downloaded files in either text or binary format based on the 'download_version' attribute.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing two lists - downloaded URLs and failed URLs.\n",
    "        \"\"\"\n",
    "        downloaded_urls = []\n",
    "        failed_urls = []\n",
    "        invalid_content_type_urls = []\n",
    "\n",
    "        print(f\"Start downloading {len(self.urls)} files using URLs...\")\n",
    "        for url in self.urls:\n",
    "            file_name = os.path.basename(url)\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    # Get the content type from the response headers\n",
    "                    content_type = response.headers.get('content-type', '').lower()\n",
    "\n",
    "                    if 'pdf' in content_type or 'text/plain' in content_type:\n",
    "                        if self.download_version == 'txt':\n",
    "                            file_path = os.path.join(self.output_path, file_name)\n",
    "                            with open(file_path, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "                                f.write(response.text)\n",
    "                        else:\n",
    "                            file_path = os.path.join(self.output_path, file_name)\n",
    "                            with open(file_path, 'wb') as f:\n",
    "                                f.write(response.content)\n",
    "                        print(f\"File downloaded: {file_name}\")\n",
    "                        downloaded_urls.append(url)\n",
    "                    else:\n",
    "                        print(f\"Invalid content type for {file_name}. Content type: {content_type}\")\n",
    "                        invalid_content_type_urls.append(url)\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "                    failed_urls.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file_name}: {e}\")\n",
    "                failed_urls.append(url)\n",
    "\n",
    "        return downloaded_urls, failed_urls, invalid_content_type_urls\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the download_files method to start the bulk download process.\n",
    "        \"\"\"\n",
    "        self.download_files()\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Allows the object to be called like a function, executing the 'run' method.\n",
    "        \"\"\"\n",
    "        return self.run()\n",
    "        \n",
    "# Read URLs from Excel\n",
    "def read_urls_from_excel(excel_path, column_name='URL'):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path)\n",
    "        return df[column_name].dropna().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "        return []\n",
    "\n",
    "global_urls_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "# Function to initiate the file download and create the Excel summary\n",
    "if __name__ == '__main__':\n",
    "    global global_folder_path, global_urls_dict  # Use the global variables\n",
    "    global_folder_path = ''# Path to save the downloaded files\n",
    "    excel_path = '.xlsx'  # Excel containing all the URLs to be downloaded (Column name shall be URL)\n",
    "    folder_path = global_folder_path  # Use the global variable\n",
    "    urls = []\n",
    "    \n",
    "    urls = read_urls_from_excel(excel_path, column_name='URL')\n",
    "\n",
    " \n",
    "\n",
    "    # Store URLs in the global dictionary\n",
    "    global_urls_dict[folder_path] = urls\n",
    "\n",
    "    # Check if the data folder exists, if not create it\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    input_dict = {\n",
    "        'root_path': folder_path,\n",
    "        'urls': urls,\n",
    "        'download_version': 'binary'  # Download files in binary format (pdf or txt)\n",
    "    }\n",
    "    bulk_downloader = URLBulkDownload(input_dict)\n",
    "    bulk_downloader.download_files()\n",
    "    \n",
    "\n",
    "\n",
    "# The following part is for renaming the files in the standard Format and save the Project Ids of the downloaded files in a excel file\n",
    "\n",
    "# Define the mapping of phrases to their abbreviations\n",
    "filetype_mapping = {\n",
    "    'Implementation Completion Report Review': 'ICRR',\n",
    "    'Implementation Completion and Results Report': 'ICR',\n",
    "    'Project Performance Assessment Report': 'PPAR',\n",
    "    'Project Appraisal Document': 'PAD',\n",
    "    'Systematic Country Diagnostic': 'SCD',\n",
    "    'Implementation Status and Results Report': 'ISR',\n",
    "    'Program Document': 'PD',\n",
    "    'Environmental and Social Management Plan': 'ESMP',\n",
    "    'Country Assistance Strategy Document': 'CAS',\n",
    "    'Country Partnership Framework': 'CPF',\n",
    "    'IEG Evaluation': 'CPE',\n",
    "    'Country Engagement Note': 'CEN',\n",
    "    'Country Re-engagement Note': 'CREN',\n",
    "    'Interim Strategy Note': 'ISN',\n",
    "    'Country Program Evaluation': 'CPE',\n",
    "    'ASA': 'ASA',\n",
    "    'CAS Completion Report Review': 'CLRR',\n",
    "    'Project Paper': 'PP',\n",
    "    'Restructing Paper': 'RES'\n",
    "}\n",
    "\n",
    "# Iterate through folder paths and their associated URLs\n",
    "for folder_path, urls in global_urls_dict.items():\n",
    "    for url in urls:\n",
    "        # Extract filename from URL\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, \"URL_downloads\", filename)\n",
    "\n",
    "        # Initialize content variable\n",
    "        content = \"\"\n",
    "\n",
    "        # Read the content of the file\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "        # Handle PDF files using PyPDF2\n",
    "        if file_extension == '.pdf':\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                content = ''\n",
    "                start = 0  # Specify the start page index\n",
    "                end = 5    # Specify the end page index\n",
    "                pdf_pages = pdf.pages[start:end]\n",
    "\n",
    "                for pdf_page in pdf_pages:\n",
    "                    page_text = ''\n",
    "\n",
    "                    # Extract header content (top portion of the page)\n",
    "                    header_height = 100  # Adjust this value based on your PDF's header height\n",
    "                    header_area = pdf_page.crop((0, 0, pdf_page.width, header_height))\n",
    "                    header_text = header_area.extract_text()\n",
    "\n",
    "                    # Extract footer content (bottom portion of the page)\n",
    "                    footer_height = 100  # Adjust this value based on your PDF's footer height\n",
    "                    footer_area = pdf_page.crop((0, pdf_page.height - footer_height, pdf_page.width, pdf_page.height))\n",
    "                    footer_text = footer_area.extract_text()\n",
    "\n",
    "                    # Extract main content (excluding header and footer)\n",
    "                    main_area = pdf_page.crop((0, header_height, pdf_page.width, pdf_page.height - footer_height))\n",
    "                    main_text = main_area.extract_text()\n",
    "\n",
    "                    # Combine header, main, and footer content\n",
    "                    page_text = f\"{header_text}\\n\\n{main_text}\\n\\n{footer_text}\\n\\n\\x0c\\n\\n\"\n",
    "\n",
    "                    content += page_text\n",
    "            \n",
    "        else:\n",
    "            with open(file_path, \"r\") as text_file:\n",
    "                content = text_file.read()\n",
    "\n",
    "\n",
    "        # Extract \"projectid\" from the content\n",
    "        projectid_matches = re.findall(r'P\\d{6}', content)\n",
    "        extracted_projectid = projectid_matches[0] if projectid_matches else \"\"\n",
    "\n",
    "        # Extract \"filetype\" from the filename or content\n",
    "        filetype_match = re.search(r'-(ICRR|ICR|PPAR|PAD|SCD|ISR|PD|ESMP|CAS|CPF|CPE|CEN|CREN|ISN|CPE|ASA|CLRR|PP)-', url)\n",
    "\n",
    "        # If no direct match from filename, check content\n",
    "        if not filetype_match:\n",
    "            patterns = [r'Report No:\\s*(ICRR|RES|ICR|PPAR|PAD|SCD|ISR|PD|ESMP|CAS|CPF|CPE|CEN|CREN|ISN|CPE|ASA|CLRR|PP)',\n",
    "                       r'Report Number:\\s*(ICRR|RES|ICR|PPAR|PAD|SCD|ISR|PD|ESMP|CAS|CPF|CPE|CEN|CREN|ISN|CPE|ASA|CLRR|PP)',\n",
    "                       r'Report Number\\s*:\\s*(ICRR|RES|ICR|PPAR|PAD|SCD|ISR|PD|ESMP|CAS|CPF|CPE|CEN|CREN|ISN|CPE|ASA|CLRR|PP)',\n",
    "                       r'Report No.:\\s*(ICRR|RES|ICR|PPAR|PAD|SCD|ISR|PD|ESMP|CAS|CPF|CPE|CEN|CREN|ISN|CPE|ASA|CLRR|PP)'\n",
    "                      ]\n",
    "            for pattern in patterns:\n",
    "                filetype_match = re.search(pattern, content)\n",
    "                if filetype_match:\n",
    "                    break  # Stop searching if a match is found\n",
    "\n",
    "        # If still no match, check for specific phrases in the content\n",
    "        if not filetype_match:\n",
    "            phrase_frequencies = {phrase: content.lower().count(phrase.lower()) for phrase in filetype_mapping.keys()}\n",
    "            most_common_phrase = max(phrase_frequencies, key=phrase_frequencies.get)\n",
    "\n",
    "            if phrase_frequencies[most_common_phrase] > 0:\n",
    "                extracted_filetype = filetype_mapping[most_common_phrase]\n",
    "            else:\n",
    "                extracted_filetype = \"UNKNOWN\"\n",
    "        else:\n",
    "            extracted_filetype = filetype_match.group(1)  # Extracted directly from the match\n",
    "\n",
    "        # Extract \"guid\" from the URL\n",
    "        guid_match = re.search(r'/(\\d+)/', urlparse(url).path)\n",
    "        extracted_guid = guid_match.group(1) if guid_match else \"\"\n",
    "\n",
    "\n",
    "        # Rename the file if necessary\n",
    "        if extracted_projectid and extracted_guid and extracted_filetype:\n",
    "            # Get the original file extension\n",
    "            _, original_extension = os.path.splitext(filename)\n",
    "\n",
    "            # Construct the new filename\n",
    "            new_filename = f\"{extracted_projectid}_{extracted_filetype}_{extracted_guid}{original_extension}\"\n",
    "\n",
    "            # Construct the paths for renaming\n",
    "            renamed_path = os.path.join(folder_path, \"URL_downloads\", new_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(file_path, renamed_path)\n",
    "\n",
    "\n",
    "# Provide the folder path where downloaded files are located\n",
    "folder_path2 = f\"{folder_path}/URL_downloads\"\n",
    "\n",
    "# Provide the folder path where you want to save the Excel file\n",
    "output_folder = f\"{folder_path}\"\n",
    "\n",
    "## Create a new Excel workbook\n",
    "workbook = Workbook()\n",
    "sheet = workbook.active\n",
    "sheet.title = \"Downloaded Files\"  # Set the sheet name\n",
    "\n",
    "# Set the column headers\n",
    "sheet[\"A1\"] = \"Proj.Id\"\n",
    "sheet[\"B1\"] = \"File Type\"\n",
    "\n",
    "# Initialize row counter\n",
    "row_num = 2\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path2)\n",
    "\n",
    "# Iterate through the files and extract project ID and file type\n",
    "for filename in file_list:\n",
    "    if filename != \".DS_Store\" and \"_\" in filename and \".\" in filename:\n",
    "        parts = filename.split(\"_\")\n",
    "        project_id = parts[0]\n",
    "        file_type = parts[1]\n",
    "        sheet.cell(row=row_num, column=1, value=project_id)\n",
    "        sheet.cell(row=row_num, column=2, value=file_type)\n",
    "        row_num += 1\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the Excel workbook in the output folder\n",
    "excel_filename = os.path.join(output_folder, f\"Downloaded_Project_IDs.xlsx\")\n",
    "workbook.save(excel_filename)\n",
    "\n",
    "print(f\"Download Summary saved as '{excel_filename}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
